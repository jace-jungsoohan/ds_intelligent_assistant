from typing import Dict, Any, List
from langchain_google_vertexai import VertexAIEmbeddings, ChatVertexAI
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from app.core.config import settings

class RetrievalAgent:
    def __init__(self):
        # 1. Initialize Embeddings & LLM
        self.embeddings = VertexAIEmbeddings(
            model_name="textembedding-gecko@003",
            project=settings.PROJECT_ID,
            location=settings.LOCATION
        )
        self.llm = ChatVertexAI(
            model_name="gemini-2.5-flash",
            project=settings.PROJECT_ID,
            location=settings.LOCATION,
            temperature=0.2
        )
        
        # 2. Key Term Definitions (Glossary) provided by user
        # In a real scenario, this could be loaded from files.
        self.glossary_texts = [
            """
            [ìš©ì–´ ì •ì˜] ìš´ì†¡ ê±´ìˆ˜
            - ì˜ë¯¸: ì¡°íšŒê¸°ê°„ ë™ì•ˆ ìš´ì†¡ ì™„ë£Œë˜ê±°ë‚˜ ìš´ì†¡ ì¤‘ì¸ ìš´ì†¡ ë¬¼ëŸ‰ì˜ ì´ í•©ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
            """,
            """
            [ìš©ì–´ ì •ì˜] ì¶œê³  ê±´ìˆ˜
            - ì˜ë¯¸: ì¡°íšŒê¸°ê°„ ë™ì•ˆ ì¶œê³ ê°€ ëœ ìš´ì†¡ ë¬¼ëŸ‰ì˜ ì´ í•©ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
            - ì£¼ì˜ì‚¬í•­: ì¡°íšŒê¸°ê°„ ì´ì „ì— ì¶œê³ ëœ ê±´ì€ í•©ê³„ì—ì„œ ì œì™¸ë©ë‹ˆë‹¤.
            """,
            """
            [ìš©ì–´ ì •ì˜] ì¼íƒˆë¥  (Deviation Rate)
            - ì˜ë¯¸: ì „ì²´ ì„¼ì‹± íšŸìˆ˜(ë¡œê·¸ ìˆ˜) ëŒ€ë¹„ ì¶©ê²© ì´ìŠˆ ë°œìƒ ë¹„ìœ¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
            - ê³µì‹: (ì¶©ê²© ì´ë²¤íŠ¸ ë°œìƒ íšŸìˆ˜ / ì „ì²´ ë¡œê·¸ ìˆ˜) * 100
            """
        ]
        
        # 3. Create Vector Store (In-Memory FAISS)
        # This runs once on startup.
        print("ğŸ—ï¸ RetrievalAgent: Building Vector Store...")
        try:
            self.vector_store = FAISS.from_texts(
                texts=self.glossary_texts,
                embedding=self.embeddings
            )
            self.retriever = self.vector_store.as_retriever(search_kwargs={"k": 1})
            print("âœ… RetrievalAgent: Vector Store ready.")
        except Exception as e:
            print(f"âŒ RetrievalAgent: Vector Store failed to build: {e}")
            self.retriever = None

        # 4. RAG Prompt
        self.prompt = ChatPromptTemplate.from_template("""
        You are a helpful assistant explaining logistics terms based on the provided context.
        Use the following pieces of retrieved context to answer the user's question.
        If the answer is not in the context, say that you don't know based on the provided documents.
        
        Context:
        {context}
        
        Question: {question}
        
        Answer (in Korean):
        """)
        
        if self.retriever:
            self.chain = (
                {"context": self.retriever, "question": RunnablePassthrough()}
                | self.prompt
                | self.llm
                | StrOutputParser()
            )
        else:
            self.chain = None

    def process_query(self, question: str, chat_history: list = None) -> Dict[str, Any]:
        """
        Retrieves relevant documents and answers the question.
        """
        if not self.chain:
            return {
                "question": question,
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í•˜ì—¬ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "source_documents": []
            }
            
        try:
            # Execute RAG Chain
            answer = self.chain.invoke(question)
            
            # Retrieve source docs manually for metadata (optional)
            docs = self.retriever.get_relevant_documents(question)
            source_contents = [doc.page_content.strip() for doc in docs]
            
            return {
                "question": question,
                "answer": answer,
                "source_documents": source_contents
            }
        except Exception as e:
            return {
                "question": question,
                "answer": f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}",
                "source_documents": []
            }

# Singleton Instance
retrieval_agent = RetrievalAgent()
