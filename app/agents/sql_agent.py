
from typing import Any, Dict
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_google_vertexai import ChatVertexAI
from app.core.config import settings
from packages.bq_wrapper.schema import get_table_info
from packages.bq_wrapper.client import BigQueryWrapper




# Initialize Vertex AI Chat Model
# Ensure your environment has Google Cloud credentials set up

try:



    llm = ChatVertexAI(
        model_name="gemini-2.5-flash",
        project=settings.PROJECT_ID,
        location=settings.LOCATION,
        temperature=0,
        max_output_tokens=2048
    )
except Exception as e:
    print(f"Warning: Could not initialize Vertex AI: {e}")
    llm = None

bq_client = BigQueryWrapper()

# 1. SQL Generation Step
template_sql_gen = """
You are a BigQuery expert for a logistics company named Willog.
Your goal is to answer user questions by generating a valid Standard SQL query.

Dataset: `willog-prod-data-gold.rag`
 
**Failure Handling**:
- If the user's question is ambiguous (e.g., uses undefined terms like "ë°°ì†¡ ê±´ìˆ˜" without context) or requires more details (e.g., "Show me data" without date/route), do NOT generate SQL.
- Instead, output: `CLARIFICATION_NEEDED: <Reason and Question to user>`
- Example: `CLARIFICATION_NEEDED: "ë°°ì†¡ ê±´ìˆ˜"ê°€ ì •í™•íˆ ì–´ë–¤ ì˜ë¯¸ì¸ê°€ìš”? 'ì¶œê³  ê±´ìˆ˜'(ì¶œë°œ ê¸°ì¤€)ì¸ê°€ìš”, ì•„ë‹ˆë©´ 'ìš´ì†¡ ê±´ìˆ˜'(ìš´ì†¡ ì¤‘ í¬í•¨)ì¸ê°€ìš”?`

**IMPORTANT**: You must use the FULLY QUALIFIED TABLE NAMES provided below (e.g. `willog-prod-data-gold.rag.mart_logistics_master`). NEVER use placeholders like `your_table_name` or `dataset.table`.

Available tables (always use fully qualified names with backticks):

1. `willog-prod-data-gold.rag.mart_logistics_master` (Fact Table)
   - Purpose: Master transport stats, volume, damage rates, RISK LEVELS, FATIGUE.
   - Columns: 
     - code (STRING): Shipment ID
     - departure_date (DATE): Partition Key - use for time filtering (e.g., "ì´ë²ˆ ë‹¬", "ìµœê·¼ 1ì£¼ì¼")
     - arrival_date (DATE): Arrival Date (CRITICAL for "ìš´ì†¡ ê±´ìˆ˜" / active shipments queries)
     - destination (STRING): Port code (e.g., 'CNSHG')
     - product (STRING)
     - transport_mode (STRING): 'air', 'truck', 'ocean+ferry', 'ocean+rail' (Note: Raw data is lowercase. 'ocean' often appears in composites.)
     - package_type (STRING): Packaging type
     - cumulative_shock_index (FLOAT): "Fatigue" or "Cumulative Stress" score
     - risk_level (STRING): 'Low', 'Medium', 'High', 'Critical'
     - temp_excursion_duration_min (INT64): Minutes outside valid temp range
     - is_damaged (BOOL): Damage flag
     - receive_name (STRING): Transport Route Name (Mapped from 'receiver_name'). e.g. 'Customer A'. Use for "ìš´ì†¡ê²½ë¡œ".

2. `willog-prod-data-gold.rag.mart_sensor_detail` (Big Data / Granular)
   - Purpose: Dynamic Threshold Queries (e.g. "Shock > 7G"), Multi-variable Correlation, Directional Analysis.
   - Columns: 
     - event_date (DATE): Partition Key - use for time filtering
     - event_timestamp (TIMESTAMP)
     - code (STRING): Shipment ID (Join Key)
     - destination (STRING): Destination port code.
     - location_fin_corrected (STRING): Transport Segment / Corrected Location Name. Use for "ìš´ì†¡êµ¬ê°„".
     - destination_country (STRING): 'China', 'Japan', 'Vietnam', 'Korea', 'USA', 'Other'
     - transport_mode (STRING): Copied from master.
     - shock_g (FLOAT), temperature (FLOAT), humidity (FLOAT)
     - acc_x, acc_y, acc_z (FLOAT): Directional acceleration
     - tilt_x, tilt_y (FLOAT): Tilt angles
     - lat, lon (FLOAT): Geolocation

3. `willog-prod-data-gold.rag.mart_risk_heatmap` (Geospatial)
   - Purpose: "Heatmap", "Risk Map", "Where do shocks occur?".
   - Columns: lat_center, lon_center, location_label, risk_score, high_impact_events

4. `willog-prod-data-gold.rag.mart_quality_matrix` (Benchmarking)
   - Purpose: Compare Performance (A vs B), Benchmarking Packaging/Routes.
   - Columns: transport_mode, package_type, route, damage_rate, avg_fatigue_score, safety_score

Scenario Guidelines (Whitepaper Analytics):
- **Fatigue/Stress**: Query `cumulative_shock_index` from `mart_logistics_master`.
- **Benchmarking/Comparison**: Query `mart_quality_matrix`.
- **Composite Conditions (e.g. Temp < 0 & Shock > 5)**: Query `mart_sensor_detail`.
- **Country filtering**: Use `destination_country` in `mart_sensor_detail` (e.g., WHERE destination_country = 'China').
- **Transport Mode**: Use LOWERCASE values ('ocean', 'air', 'truck') or `LIKE` for safety (e.g. `WHERE transport_mode LIKE 'ocean%'`).
- **Time filtering**: Use `departure_date` or `event_date` with DATE functions:
  - "ì´ë²ˆ ë‹¬": `WHERE event_date >= DATE_TRUNC(CURRENT_DATE(), MONTH)`
  - "ìµœê·¼ 1ì£¼ì¼": `WHERE event_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)`
  - "ì§€ë‚œë‹¬": `WHERE event_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH) AND LAST_DAY(DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))`
- **Ambiguity Prevention**: ALWAYS use table aliases (e.g. `t1.code`, `t2.destination`) when joining tables. Columns `code` and `destination` exist in multiple tables execution will fail if not qualified.
- **Data Quality**: When querying risk scores (`risk_score`) or damage rates, ALWAYS filter out zero or NULL values (e.g., `WHERE risk_score > 0`) to avoid meaningless results.
- **Uniqueness**: CRITICAL! When ranking items (e.g. 'Top 5'), YOU MUST use `DISTINCT code` or `GROUP BY code`. Duplicate rows may exist in the source.
- **Metric Definitions**:
  - "ì¶œê³  ê±´ìˆ˜" (Departed Shipments): Shipments started in period. Query `mart_logistics_master`.
    -> `SELECT COUNT(DISTINCT code) FROM mart_logistics_master WHERE departure_date BETWEEN 'START' AND 'END'`
   - "ìš´ì†¡ ê±´ìˆ˜" (Active/Total Shipments): Shipments active during the period. Includes those generated before but still in transit or arrived during period.
     -> CRITICAL: DO NOT use `departure_date BETWEEN`.
     -> Correct Logic: `departure_date <= 'END' AND (arrival_date >= 'START' OR arrival_date IS NULL)`
     -> Correct Logic: `departure_date <= 'END' AND (arrival_date >= 'START' OR arrival_date IS NULL)`
     -> Query: `SELECT COUNT(DISTINCT code) FROM mart_logistics_master WHERE departure_date <= 'END' AND (arrival_date >= 'START' OR arrival_date IS NULL)`
   - "ì¼íƒˆë¥ " (Deviation Rate/Excursion Rate):
     -> If Aggregated (Daily/Monthly): `SAFE_DIVIDE(COUNTIF(risk_level IN ('High', 'Critical')), COUNT(*))` in Master.
     -> If by Shipment/Code ("ê´€ë¦¬ë²ˆí˜¸ë³„ ì¼íƒˆë¥ "): Calculate Sensor Log Excursion Rate.
        Query: `SELECT code, SAFE_DIVIDE(COUNTIF(shock_g >= 5 OR temperature < 2 OR temperature > 8), COUNT(*)) as excursion_rate FROM mart_sensor_detail GROUP BY code`

Code Mapping Guide (Fuzzy Matching & Entity Resolution):
- Shanghai, Sanghai, Sanghi, Shanhai, ìƒí•´, ìƒí•˜ì´, SH -> 'CNSHG' (or destination LIKE '%Shanghai%')
- Osaka, Osaca, Osk, ì˜¤ì‚¬ì¹´, ì˜¤ì‚¬ì¹´í•­ -> 'JPOSA'
- Rizhao, Rizo, ì¼ì¡°, ë¦¬ìì˜¤ -> 'CNRZH'
- Lianyungang, Lianyun, ì—°ìš´í•­ -> 'CNLYG'
- Ningbo, Ningpo, ë‹ë³´ -> 'CNNBG'
- Hochiminh, HCMC, VN SGN, í˜¸ì¹˜ë¯¼ -> 'VNSGN'
- Haiphong, VN HPH, í•˜ì´í -> 'VNHPH'
- Incheon, ICN, ì¸ì²œ -> 'KRICN'
- Busan, Pusan, ë¶€ì‚° -> 'KRPUS'
- "ì¤‘êµ­", "China", "CN" -> destination_country = 'China' OR destination LIKE 'CN%'
- "ë² íŠ¸ë‚¨", "Vietnam", "VN" -> destination_country = 'Vietnam' OR destination LIKE 'VN%'
- "ì¼ë³¸", "Japan", "JP" -> destination_country = 'Japan' OR destination LIKE 'JP%'
- "ë¯¸êµ­", "USA", "US" -> destination_country = 'USA' OR destination LIKE 'US%'
- "ë°°ì†¡ ê±´ìˆ˜", "ë°°ì†¡ëŸ‰" -> Same as "ì¶œê³  ê±´ìˆ˜" (Departed Shipments)
- "ë¬¼ë™ëŸ‰" -> Can be "ì¶œê³  ê±´ìˆ˜" or "ìš´ì†¡ ê±´ìˆ˜" depending on context, default to "ì¶œê³  ê±´ìˆ˜".
- "ìš´ì†¡ê²½ë¡œ", "ê²½ë¡œ" -> Use `receive_name` column.
- "ìš´ì†¡êµ¬ê°„", "êµ¬ê°„" -> Use `location_fin_corrected` column in `mart_sensor_detail`.

Example SQLs (Few-shot Learning):
1. "ğŸ›³ï¸ í•´ìƒ ìš´ì†¡ ì¤‘ 5G ì´ìƒ ì¶©ê²© ë°œìƒ ë¹„ìœ¨" (Ratio Calculation)
SELECT
    t2.transport_mode,
    COUNTIF(t1.shock_g >= 5) as high_shock_count,
    COUNT(*) as total_sensor_readings,
    SAFE_DIVIDE(COUNTIF(t1.shock_g >= 5), COUNT(*)) as high_shock_ratio
    SAFE_DIVIDE(COUNTIF(t1.shock_g >= 5), COUNT(*)) as high_shock_ratio
FROM `willog-prod-data-gold.rag.mart_sensor_detail` t1
JOIN `willog-prod-data-gold.rag.mart_logistics_master` t2 ON t1.code = t2.code
WHERE t2.transport_mode LIKE 'ocean%' -- Use LIKE for safety or 'ocean'
GROUP BY 1

WHERE t2.transport_mode LIKE 'ocean%' -- Use LIKE for safety or 'ocean'
GROUP BY 1

2. "ìš´ì†¡ê²½ë¡œ ë³„ ë„ì°©ì§€ íë¦„" (Sankey Chart + Active Shipments)
-- Flow Analysis + "ìš´ì†¡ ê±´ìˆ˜" (Active) logic
SELECT
    t1.receive_name as source_node,
    t1.destination as target_node,
    COUNT(DISTINCT t1.code) as flow_count
FROM `willog-prod-data-gold.rag.mart_logistics_master` t1
WHERE 
    t1.departure_date <= '2025-12-07' 
    AND (t1.arrival_date >= '2025-12-01' OR t1.arrival_date IS NULL)
GROUP BY 1, 2
ORDER BY 3 DESC

3. "ë² íŠ¸ë‚¨í–‰ í™”ë¬¼ ì¤‘ ìŠµë„ ì´íƒˆ êµ¬ê°„" (Route/Location Analysis)
SELECT lat, lon, COUNT(*) as excursion_count
FROM `willog-prod-data-gold.rag.mart_sensor_detail`
WHERE destination LIKE '%VN%' OR destination = 'VNSGN' -- ERROR: Destination not in sensor_detail
-- CORRECT APPROACH:
-- SELECT t1.lat, t1.lon, COUNT(*) 
-- FROM `willog-prod-data-gold.rag.mart_sensor_detail` t1
-- JOIN `willog-prod-data-gold.rag.mart_logistics_master` t2 ON t1.code = t2.code
-- WHERE t2.destination LIKE '%VN%' ...

3. "ì´ë²ˆ ë‹¬ ì¤‘êµ­ì—ì„œ ì˜í•˜ ì˜¨ë„ ì¶©ê²© ê±´ìˆ˜" (Location + Sensor Condition)
SELECT
    COUNT(*) as shock_count_below_zero
FROM `willog-prod-data-gold.rag.mart_sensor_detail` t1
JOIN `willog-prod-data-gold.rag.mart_logistics_master` t2 ON t1.code = t2.code
WHERE
    t2.destination LIKE '%China%' OR t2.destination IN ('CNSHG', 'CNNBG', 'CNRZH', 'CNLYG')
    AND t1.temperature < 0
    AND t1.shock_g > 0 
    AND t1.event_date BETWEEN DATE_TRUNC(CURRENT_DATE(), MONTH) AND CURRENT_DATE()

4. "â„ï¸ 60ë¶„ì´ìƒ ì§€ì†ëœ ì˜í•˜ ì˜¨ë„ì—ì„œ ë°œìƒí•œ ì¶©ê²© ê±´ìˆ˜" (Duration + Complex Condition)
-- 'Duration' queries usually refer to 'temp_excursion_duration_min' in master table.
SELECT
    COUNT(*) as shock_count
FROM `willog-prod-data-gold.rag.mart_sensor_detail` t1
JOIN `willog-prod-data-gold.rag.mart_logistics_master` t2 ON t1.code = t2.code
WHERE
    t1.temperature < 0
    AND t2.temp_excursion_duration_min >= 60 -- Use pre-calculated duration from master
    AND t1.shock_g > 0

5. "âš ï¸ ëˆ„ì  í”¼ë¡œë„ Top 5 ìš´ì†¡ ê±´" (Ranking with DISTINCT)
SELECT DISTINCT
    code,
    cumulative_shock_index
FROM `willog-prod-data-gold.rag.mart_logistics_master`
WHERE cumulative_shock_index IS NOT NULL
ORDER BY cumulative_shock_index DESC
LIMIT 5

6. "ì´ë²ˆ ë‹¬ ìš´ì†¡ ê±´ìˆ˜ ë° ì¶œê³  ê±´ìˆ˜ ë¹„êµ" (Active vs Departed)
SELECT
    COUNT(DISTINCT CASE WHEN t1.departure_date BETWEEN '2025-11-01' AND '2025-11-30' THEN t1.code END) as departed_count,
    COUNT(DISTINCT CASE WHEN t1.departure_date <= '2025-11-30' AND (t1.arrival_date >= '2025-11-01' OR t1.arrival_date IS NULL) THEN t1.code END) as active_transport_count
FROM `willog-prod-data-gold.rag.mart_logistics_master` t1
WHERE t1.departure_date <= '2025-11-30'

7. "ë„ì°©ì§€ë³„ ìš´ì†¡ ê±´ìˆ˜ ë¹„ì¤‘ ì¶”ì´" (Daily Active Ratio Trend)
-- Use mart_sensor_detail for DAILY active status. Window function calculates daily share.
SELECT
    event_date,
    destination,
    COUNT(DISTINCT code) as active_count,
    SAFE_DIVIDE(COUNT(DISTINCT code), SUM(COUNT(DISTINCT code)) OVER (PARTITION BY event_date)) * 100 as share_percentage
FROM `willog-prod-data-gold.rag.mart_sensor_detail`
WHERE event_date BETWEEN '2025-11-01' AND '2025-11-30'
GROUP BY 1, 2
ORDER BY 1, 2

8. "ìš´ì†¡ ê²½ë¡œ íë¦„ ë¶„ì„" (Sankey Flow Chart)
-- To trigger Sankey visualization, ALIAS columns as `source_node` and `target_node`.
SELECT
    'Korea' as source_node,
    destination as target_node,
    COUNT(DISTINCT code) as flow_count
FROM `willog-prod-data-gold.rag.mart_sensor_detail`
WHERE event_date BETWEEN '2025-11-01' AND '2025-11-30'
GROUP BY 1, 2
ORDER BY 3 DESC

**Failure Handling (Clarification)**:
- If DATE RANGE is missing for trend queries ("ì¶”ì´ ì•Œë ¤ì¤˜"), ask: `CLARIFICATION_NEEDED: ì–¸ì œë¶€í„° ì–¸ì œê¹Œì§€ì˜ ë°ì´í„°ë¥¼ ì¡°íšŒí• ê¹Œìš”?`
- If METRIC is unclear ("ë¬¼ë™ëŸ‰ ì•Œë ¤ì¤˜"), ask: `CLARIFICATION_NEEDED: 'ì¶œê³  ê±´ìˆ˜'(ì¶œë°œ ê¸°ì¤€)ë¥¼ ì›í•˜ì‹œë‚˜ìš”, ì•„ë‹ˆë©´ 'ìš´ì†¡ ê±´ìˆ˜'(ìš´ì†¡ ì¤‘ í¬í•¨)ë¥¼ ì›í•˜ì‹œë‚˜ìš”?`
- For Flow/Connection queries ("~ë³„ ~", "íë¦„", "ì—°ê²°"), ALWAYS return `source_node` and `target_node` columns to show Sankey Chart.
- For Ratio Trend queries ("ë¹„ì¤‘ ì¶”ì´", "ì ìœ ìœ¨ ì¶”ì´"), ALWAYS calculate `SAFE_DIVIDE(..., SUM(...) OVER(PARTITION BY date)) * 100 AS share_percentage` to trigger Stacked Bar Chart.


Previous Conversation Context:
{chat_history}

Question: {question}
SQL Query:
"""

prompt_sql_gen = ChatPromptTemplate.from_template(template_sql_gen)

def get_schema(_):
    return get_table_info()



if llm:
    sql_generator_chain = (
        prompt_sql_gen
        | llm
        | StrOutputParser()
    )
else:
    # Mock chain if LLM is not available
    class MockSQLChain:
        def invoke(self, input_dict):
            return "```sql\n-- Mock SQL (LLM unavailable)\nSELECT * FROM view_transport_stats LIMIT 10\n```"
    sql_generator_chain = MockSQLChain()

# 2. Response Synthesis Prompt
template_synthesis = """
You are a helpful assistant for Willog logistics.
Based on the SQL query result, provide a natural language answer to the user's question.
Answer in Korean (í•œêµ­ì–´).

User Question: {question}
SQL Query: {sql}
Query Result: {result}

If the result is empty or says "(empty)", say "í•´ë‹¹ ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
Otherwise, summarize the key findings from the result.
Do NOT say there is no data if values are present.

Answer:
"""

prompt_synthesis = ChatPromptTemplate.from_template(template_synthesis)

if llm:
    synthesis_chain = prompt_synthesis | llm | StrOutputParser()
else:
    synthesis_chain = None

class SQLAgent:
    def __init__(self):
        self.chain = sql_generator_chain
        self.synthesis_chain = synthesis_chain
    
    def process_query(self, question: str, chat_history: list = None) -> Dict[str, Any]:
        from datetime import date
        current_date = date.today().isoformat()
        
        # Format history logic
        history_str = ""
        if chat_history:
            recent_history = chat_history[-6:] # Context window 6
            for msg in recent_history:
                role = "User" if msg.get("role") == "user" else "Assistant"
                content = msg.get("content", "")
                history_str += f"{role}: {content}\n"

        # 1. Generate SQL
        generated_sql = self.chain.invoke({
            "question": question, 
            "current_date": current_date,
            "chat_history": history_str
        })
        
        print(f"DEBUG: Generated SQL for '{question}': [{generated_sql}]") # Debug log
        
        clean_sql = generated_sql.replace("```sql", "").replace("```", "").strip()
        
        if "CLARIFICATION_NEEDED:" in clean_sql:
            return {
                "question": question,
                "generated_sql": "",
                "result": None,
                "natural_response": clean_sql.replace("CLARIFICATION_NEEDED:", "").strip(),
                "error": None
            }

        if not clean_sql:
            return {
                "question": question,
                "generated_sql": "",
                "result": None,
                "natural_response": "ì§ˆë¬¸ì„ ì´í•´í•˜ëŠ”ë° ì–´ë ¤ì›€ì´ ìˆìŠµë‹ˆë‹¤. ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ(ê¸°ê°„, ì¡°ê±´ ë“±) ë§ì”€í•´ ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”?",
                "error": "Empty SQL generated"
            }
        
        # 2. Execute SQL against BigQuery
        result_df = None
        error = None
        try:
            if bq_client.client:
                print(f"DEBUG: Executing query on BigQuery...")
                result_df = bq_client.run_query(clean_sql)
                print(f"DEBUG: Query executed. Result shape: {result_df.shape if result_df is not None else 'None'}")
            else:
                error = "BigQuery Client is not initialized (client object is None)."
                print(f"DEBUG: {error}")
        except Exception as e:
            error = str(e)
            print(f"DEBUG: Query execution failed: {error}")

        # 3. Synthesize natural language response
        natural_response = None
        if result_df is not None and self.synthesis_chain:
            try:
                # Convert DataFrame to string for LLM
                result_str = result_df.to_string() if not result_df.empty else "(empty)"
                print(f"DEBUG: Synthesis Input Result:\n{result_str}")
                natural_response = self.synthesis_chain.invoke({
                    "question": question,
                    "sql": clean_sql,
                    "result": result_str
                })
            except Exception as e:
                natural_response = f"ê²°ê³¼ í•´ì„ ì¤‘ ì˜¤ë¥˜: {e}"
        elif error:
            natural_response = f"ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error}"
        elif result_df is None:
            # Fallback for unexpected None result without explicit error
            natural_response = (
                f"âš ï¸ ë°ì´í„° ì¡°íšŒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\n"
                f"SQLì€ ìƒì„±ë˜ì—ˆìœ¼ë‚˜ BigQuery ì‹¤í–‰ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n"
                f"ë””ë²„ê·¸ ì •ë³´:\n"
                f"- SQL: `{clean_sql}`\n"
                f"- BQ Client Status: {'Active' if bq_client.client else 'Inactive'}"
            )

        return {
            "question": question,
            "generated_sql": clean_sql,
            "result": result_df,
            "natural_response": natural_response,
            "error": error
        }

agent = SQLAgent()
